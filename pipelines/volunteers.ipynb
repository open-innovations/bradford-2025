{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volunteering Data\n",
    "\n",
    "Data extracted from Rosterfy and processes below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import date, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import petl as etl\n",
    "from utils import make_cumulative, make_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "induction_launch = date.fromisoformat('2024-09-12')\n",
    "first_event = date.fromisoformat('2024-08-16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up references to paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path('../')\n",
    "DATA = ROOT / 'data/published/volunteers'\n",
    "_TARGET = ROOT / 'src/themes/volunteers/_data'\n",
    "PEOPLE = _TARGET / 'people'\n",
    "PEOPLE.mkdir(exist_ok=True, parents=True)\n",
    "SHIFTS = _TARGET / 'shifts'\n",
    "SHIFTS.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read checkpoints data from CSV process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = etl.fromcsv(\n",
    "    DATA / 'checkpoints.csv'\n",
    ").convert(\n",
    "    'date', etl.dateparser('%Y-%m-%d')\n",
    ").convert('count', int).sort(\n",
    "    key=('date', 'checkpoint')\n",
    ").recast(\n",
    "    variablefield='checkpoint', valuefield='count'\n",
    ").replaceall(\n",
    "    None, 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_updates = (\n",
    "    etl.fromcsv(DATA / 'checkpoint-updates.csv')\n",
    "    .selectin('checkpoint', ['1. Monitoring & Evaluation', '2. Sign Up to Induction', '3. Fully Inducted Volunteers'])\n",
    "    .convert('date', etl.dateparser('%Y-%m-%d'))\n",
    "    .convert('count', int)\n",
    "    .recast(variablefield='checkpoint', valuefield='count')\n",
    "    .replaceall(None, 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create summarise function. This takes a Pandas dataframe, resamples based on the desired frequency `freq`, adds cumulative counts of each column, converts to a PETL table and renames the date column based on the `title` provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise(df, freq, title):\n",
    "    return (\n",
    "        df\n",
    "            .pipe(make_time_series)\n",
    "            .resample(freq)\n",
    "            .sum()\n",
    "            .pipe(make_cumulative)\n",
    "            .reset_index()\n",
    "            .pipe(etl.fromdataframe)\n",
    "            .convert('date', datetime.date)\n",
    "            .rename({ 'date': title })\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a weekly summary of checkpoints passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints.todataframe(\n",
    ").pipe(\n",
    "    summarise, 'W-SUN', 'Week ending (Sunday)'\n",
    ").selectge('Week ending (Sunday)', induction_launch).tocsv(PEOPLE / 'checkpoints_weekly.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    checkpoint_updates\n",
    "    .todataframe()\n",
    "    .pipe(summarise, 'W-SUN', 'Week ending (Sunday)')\n",
    "    .selectge('Week ending (Sunday)', induction_launch)\n",
    "    .addfield('1. Monitoring & Evaluation cumulative (incl)', lambda r: r['1. Monitoring & Evaluation cumulative']\n",
    "                + r['2. Sign Up to Induction cumulative']\n",
    "                + r['3. Fully Inducted Volunteers cumulative'])\n",
    "    .addfield('2. Sign Up to Induction cumulative (incl)', lambda r: r['2. Sign Up to Induction cumulative']\n",
    "                + r['3. Fully Inducted Volunteers cumulative'])\n",
    "    .tocsv(PEOPLE / 'checkpoints_weekly_updates.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo = etl.fromcsv(DATA / 'geo-summary.csv')\n",
    "for geography, table in geo.facet('type').items():\n",
    "    table.tocsv(PEOPLE / f'by_geo_{geography}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = etl.fromcsv(DATA / 'demographics.csv').selectin('category', [\n",
    "    'age_range',\n",
    "])\n",
    "\n",
    "for category, table in demo.facet('category').items():\n",
    "    table.tocsv(PEOPLE / f'by_demographic_{category}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifts = etl.fromcsv(\n",
    "    DATA / 'shifts.csv'\n",
    ").convert(\n",
    "    ('demand', 'attended'), int\n",
    ").convert(\n",
    "    ('hours'), float\n",
    ").replaceall(\n",
    "    None, 0\n",
    ").aggregate(\n",
    "    key=['date', 'type'],\n",
    "    aggregation={\n",
    "        'attended': ('attended', sum),\n",
    "        'hours': ('hours', sum)\n",
    "    }\n",
    ").convert(\n",
    "    'date', date.fromisoformat\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create some summary functions with a combination of PETL and Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise(table: etl.Table, column, freq) -> etl.Table:\n",
    "    return (\n",
    "        table\n",
    "        .cut('date', 'type', column)\n",
    "        .recast(variablefield='type', valuefield=column, reducers=sum)\n",
    "        .replaceall(None, 0)\n",
    "        .todataframe()\n",
    "        .pipe(make_time_series)\n",
    "        .resample(freq)\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "        .pipe(etl.fromdataframe)\n",
    "        .convert('date', datetime.date)\n",
    "        .sort('date')\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarise(\n",
    "    shifts, 'attended', 'W-SUN'\n",
    ").selectge(\n",
    "    'date', first_event\n",
    ").rename(\n",
    "    'date', 'week_ending'\n",
    ").tocsv(SHIFTS / 'attended_by_week.csv')\n",
    "\n",
    "summarise(\n",
    "    shifts, 'hours', 'W-SUN'\n",
    ").selectge(\n",
    "    'date', first_event\n",
    ").melt(\n",
    "    'date'\n",
    ").convert(\n",
    "    'value', lambda f: round(f, 3)\n",
    ").recast(\n",
    ").rename(\n",
    "    'date', 'week_ending'\n",
    ").tocsv(SHIFTS / 'hours_by_week.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarise(\n",
    "    shifts, 'attended', 'W-SUN'\n",
    ").todataframe(\n",
    ").set_index('date').cumsum().reset_index(\n",
    ").pipe(\n",
    "    etl.fromdataframe\n",
    ").selectge(\n",
    "    'date', first_event\n",
    ").rename(\n",
    "    'date', 'week_ending'\n",
    ").tocsv(SHIFTS / 'attended_cumulative_by_week.csv')\n",
    "\n",
    "summarise(\n",
    "    shifts, 'hours', 'W-SUN'\n",
    ").todataframe(\n",
    ").set_index('date').cumsum().reset_index(\n",
    ").pipe(\n",
    "    etl.fromdataframe\n",
    ").selectge(\n",
    "    'date', first_event\n",
    ").melt(\n",
    "    'date'\n",
    ").convert(\n",
    "    'value', lambda f: round(f, 3)\n",
    ").recast(\n",
    ").rename(\n",
    "    'date', 'week_ending'\n",
    ").tocsv(SHIFTS / 'hours_cumulative_by_week.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert timestamp columns to datetime to work with OI Lume Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoints_monthly['Month ending'] = pd.to_datetime(checkpoints_monthly['Month ending']).dt.strftime('%Y-%m-%d')\n",
    "# shifts_monthly['Month ending'] = pd.to_datetime(shifts_monthly['month ending']).dt.strftime('%Y-%m-%d')\n",
    "# shifts_weekly['Month ending'] = pd.to_datetime(shifts_weekly['week ending']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# checkpoints_monthly.to_csv(os.path.join(OUT_DIR, 'checkpoints_monthly.csv'), index=False)\n",
    "# shifts_monthly.to_csv(os.path.join(OUT_DIR, 'shifts_monthly.csv'), index=False)\n",
    "# shifts_weekly.to_csv(os.path.join(OUT_DIR, 'shifts_weekly.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation = {f: (f, sum) for f in checkpoint_updates.header()[1:]}\n",
    "checkpoint_summary = dict(\n",
    "    checkpoint_updates\n",
    "    .aggregate(None, aggregation)\n",
    "    .rename({\n",
    "        '1. Monitoring & Evaluation': 'Signed up',\n",
    "        '2. Sign Up to Induction': 'Induction booked',\n",
    "        '3. Fully Inducted Volunteers': 'Induction completed'\n",
    "    })\n",
    "    .convert('Signed up', lambda f, r: f + r['Induction booked'] + r['Induction completed'], pass_row=True)\n",
    "    .convert('Induction booked', lambda f, r: f + r['Induction completed'], pass_row=True)\n",
    "    .transpose()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifts_summary = dict(\n",
    "    shifts\n",
    "    .melt(['date', 'type'])\n",
    "    .aggregate(['type', 'variable'], sum, 'value')\n",
    "    .sort('variable')\n",
    "    .convert('variable', {\n",
    "        'attended': 'volunteer shifts',\n",
    "        'hours': 'volunteer hours'\n",
    "    })\n",
    "    .addfield('title', lambda r: f'{r.type} {r.variable}')\n",
    "    .cut('title', 'value')\n",
    "    .records()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(\n",
    "    checkpoint_summary | shifts_summary,\n",
    "    open(_TARGET / 'summary.json', 'w'),\n",
    "    indent=2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data.bradford2025.co.uk-DmiwwEw1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
